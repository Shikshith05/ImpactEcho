{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPAMLsmkARPrkZZ9lSYRWo3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shikshith05/ImpactEcho/blob/main/impactechomodel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9kyYDSjuWp5z"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "def generate_impact_data(num_records=10000, fake_rate=0.1):\n",
        "    \"\"\"\n",
        "    Generate synthetic NGO impact verification data for ImpactEcho AI.\n",
        "\n",
        "    Args:\n",
        "        num_records (int): Number of total records to generate.\n",
        "        fake_rate (float): Fraction of fake or manipulated records.\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: Generated dataset with authenticity labels.\n",
        "    \"\"\"\n",
        "\n",
        "    num_fake = int(num_records * fake_rate)\n",
        "    num_real = num_records - num_fake\n",
        "\n",
        "    print(f\"Generating {num_records:,} NGO impact records ({num_real:,} real, {num_fake:,} fake)\")\n",
        "\n",
        "    ngo_ids = [f\"NGO-{i:03d}\" for i in range(1, 51)]\n",
        "    project_types = [\"Tree Plantation\", \"Education Drive\", \"Health Camp\", \"Clean Water\", \"Women Empowerment\", \"Food Distribution\"]\n",
        "    locations = [\"Delhi\", \"Bangalore\", \"Hyderabad\", \"Mumbai\", \"Kolkata\", \"Chennai\", \"Pune\"]\n",
        "\n",
        "    real_data, fake_data = [], []\n",
        "    base_time = datetime(2025, 9, 1, 10, 0, 0)\n",
        "\n",
        "    # ✅ Generate real records\n",
        "    for _ in range(num_real):\n",
        "        ngo_id = random.choice(ngo_ids)\n",
        "        project = random.choice(project_types)\n",
        "        location = random.choice(locations)\n",
        "        timestamp = base_time + timedelta(days=random.randint(0, 60))\n",
        "\n",
        "        beneficiaries_reported = random.randint(80, 500)\n",
        "        verified_beneficiaries = beneficiaries_reported - random.randint(0, 10)\n",
        "        impact_score = round(random.uniform(0.8, 1.0), 2)\n",
        "        funding_amount = random.randint(50000, 300000)\n",
        "\n",
        "        media_link = f\"https://impactecho.org/media/{ngo_id}_{project.replace(' ', '_')}.jpg\"\n",
        "\n",
        "        real_data.append([\n",
        "            timestamp.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "            ngo_id,\n",
        "            project,\n",
        "            location,\n",
        "            media_link,\n",
        "            beneficiaries_reported,\n",
        "            verified_beneficiaries,\n",
        "            funding_amount,\n",
        "            impact_score,\n",
        "            0  # is_fake\n",
        "        ])\n",
        "\n",
        "    # Generate fake records\n",
        "    for _ in range(num_fake):\n",
        "        ngo_id = random.choice(ngo_ids + [f\"FAKE-{i:03d}\" for i in range(51, 61)])\n",
        "        project = random.choice(project_types + [\"Ghost Project\", \"Phantom Relief\"])\n",
        "        location = random.choice(locations + [\"Unknown\", \"Remote Village\", \"N/A\"])\n",
        "        timestamp = base_time + timedelta(days=random.randint(0, 60))\n",
        "\n",
        "        # Fake or inconsistent metrics\n",
        "        beneficiaries_reported = random.randint(100, 1000)\n",
        "        verified_beneficiaries = max(0, beneficiaries_reported - random.randint(100, 900))\n",
        "        impact_score = round(random.uniform(0.0, 0.4), 2)\n",
        "        funding_amount = random.randint(100000, 400000)\n",
        "\n",
        "        # Broken or reused media\n",
        "        media_link = random.choice([\n",
        "            f\"https://impactecho.org/media/fake_{random.randint(1,999)}.jpg\",\n",
        "            \"https://imgur.com/fakeproof123\",\n",
        "            \"https://drive.google.com/brokenlink\"\n",
        "        ])\n",
        "\n",
        "        fake_data.append([\n",
        "            timestamp.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "            ngo_id,\n",
        "            project,\n",
        "            location,\n",
        "            media_link,\n",
        "            beneficiaries_reported,\n",
        "            verified_beneficiaries,\n",
        "            funding_amount,\n",
        "            impact_score,\n",
        "            1  # is_fake\n",
        "        ])\n",
        "\n",
        "    # Combine + shuffle\n",
        "    df = pd.DataFrame(real_data + fake_data, columns=[\n",
        "        \"timestamp\", \"ngo_id\", \"project_type\", \"location\", \"media_link\",\n",
        "        \"reported_beneficiaries\", \"verified_beneficiaries\", \"funding_amount\",\n",
        "        \"impact_score\", \"is_fake\"\n",
        "    ])\n",
        "\n",
        "    df = df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "    print(f\"\\n Data generation complete!\")\n",
        "    print(f\"Total records: {len(df):,}\")\n",
        "    print(f\"Fake records: {df['is_fake'].sum():,}\")\n",
        "    print(f\"Fake rate: {df['is_fake'].mean() * 100:.2f}%\")\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def save_to_json(df, filename=\"impact_data\"):\n",
        "    df.to_json(f\"{filename}.json\", orient=\"records\", indent=2)\n",
        "    df.to_json(f\"{filename}.jsonl\", orient=\"records\", lines=True)\n",
        "    print(f\" Saved to {filename}.json and {filename}.jsonl\")\n",
        "\n",
        "\n",
        "def display_sample(df, n=5):\n",
        "    print(f\"\\nShowing {n} random samples:\\n\")\n",
        "    print(df.sample(n).to_string(index=False))\n",
        "    print(\"\\n Summary:\\n\", df.describe())\n",
        "\n",
        "\n",
        "if __name__ == \"_main_\":\n",
        "    impact_df = generate_impact_data(num_records=5000, fake_rate=0.1)\n",
        "    display_sample(impact_df)\n",
        "    save_to_json(impact_df, \"impact_data_full\")\n",
        "\n",
        "    # Split for training/testing AI model\n",
        "    train_df = impact_df.sample(frac=0.8, random_state=42)\n",
        "    test_df = impact_df.drop(train_df.index)\n",
        "\n",
        "    save_to_json(train_df, \"impact_data_train\")\n",
        "    save_to_json(test_df, \"impact_data_test\")\n",
        "\n",
        "    print(\"\\nTraining records:\", len(train_df))\n",
        "    print(\"Testing records:\", len(test_df))\n",
        "    print(\"\\nImpactEcho synthetic dataset created successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "impact_df = generate_impact_data(num_records=5000, fake_rate=0.1)\n",
        "display_sample(impact_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRNVF_gSXRGU",
        "outputId": "790d670c-8b02-4c0e-e42f-1dfc3f07ddad"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating 5,000 NGO impact records (4,500 real, 500 fake)\n",
            "\n",
            " Data generation complete!\n",
            "Total records: 5,000\n",
            "Fake records: 500\n",
            "Fake rate: 10.00%\n",
            "\n",
            "Showing 5 random samples:\n",
            "\n",
            "          timestamp   ngo_id      project_type  location                                               media_link  reported_beneficiaries  verified_beneficiaries  funding_amount  impact_score  is_fake\n",
            "2025-09-20 10:00:00 FAKE-058 Food Distribution    Mumbai                           https://imgur.com/fakeproof123                     702                     368          254065          0.16        1\n",
            "2025-10-16 10:00:00  NGO-002   Tree Plantation Hyderabad https://impactecho.org/media/NGO-002_Tree_Plantation.jpg                     111                     106          274595          0.93        0\n",
            "2025-09-04 10:00:00  NGO-007   Education Drive     Delhi https://impactecho.org/media/NGO-007_Education_Drive.jpg                     500                     498          128658          0.94        0\n",
            "2025-10-15 10:00:00  NGO-004       Health Camp   Kolkata     https://impactecho.org/media/NGO-004_Health_Camp.jpg                     319                     316          162870          0.88        0\n",
            "2025-10-24 10:00:00  NGO-042       Health Camp Hyderabad     https://impactecho.org/media/NGO-042_Health_Camp.jpg                     140                     139          161495          0.85        0\n",
            "\n",
            " Summary:\n",
            "        reported_beneficiaries  verified_beneficiaries  funding_amount  \\\n",
            "count             5000.000000             5000.000000     5000.000000   \n",
            "mean               313.268200              270.939000   181703.407800   \n",
            "std                161.007671              139.798936    76182.449448   \n",
            "min                 80.000000                0.000000    50026.000000   \n",
            "25%                187.000000              159.000000   117414.750000   \n",
            "50%                301.000000              273.000000   181318.000000   \n",
            "75%                411.000000              386.000000   241859.750000   \n",
            "max                998.000000              867.000000   399934.000000   \n",
            "\n",
            "       impact_score     is_fake  \n",
            "count   5000.000000  5000.00000  \n",
            "mean       0.829286     0.10000  \n",
            "std        0.222929     0.30003  \n",
            "min        0.000000     0.00000  \n",
            "25%        0.830000     0.00000  \n",
            "50%        0.890000     0.00000  \n",
            "75%        0.940000     0.00000  \n",
            "max        1.000000     1.00000  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Copy dataframe to avoid modifying original\n",
        "df = impact_df.copy()\n",
        "\n",
        "# 1️⃣ Derived feature\n",
        "df['beneficiary_gap'] = df['reported_beneficiaries'] - df['verified_beneficiaries']\n",
        "\n",
        "# 2️⃣ Features & labels\n",
        "features = [\n",
        "    'reported_beneficiaries',\n",
        "    'verified_beneficiaries',\n",
        "    'beneficiary_gap',\n",
        "    'funding_amount',\n",
        "    'impact_score',\n",
        "    'ngo_id',\n",
        "    'project_type',\n",
        "    'location'\n",
        "]\n",
        "X = df[features]\n",
        "y = df['is_fake']\n",
        "\n",
        "# 3️⃣ Encode categorical columns\n",
        "categorical_cols = ['ngo_id', 'project_type', 'location']\n",
        "encoders = {}\n",
        "for col in categorical_cols:\n",
        "    le = LabelEncoder()\n",
        "    X[col] = le.fit_transform(X[col])\n",
        "    encoders[col] = le\n",
        "\n",
        "# Check preprocessed data\n",
        "print(\"Preprocessed feature sample:\")\n",
        "print(X.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJbDre1wYdam",
        "outputId": "4aac3db3-daa7-47b3-b838-2b6a3b771e5a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessed feature sample:\n",
            "   reported_beneficiaries  verified_beneficiaries  beneficiary_gap  \\\n",
            "0                     434                     427                7   \n",
            "1                     138                     131                7   \n",
            "2                     926                     410              516   \n",
            "3                     211                     203                8   \n",
            "4                     307                     306                1   \n",
            "\n",
            "   funding_amount  impact_score  ngo_id  project_type  location  \n",
            "0          200986          0.85      25             7         2  \n",
            "1          275632          0.86      59             4         5  \n",
            "2          212751          0.40       5             1         1  \n",
            "3          280288          0.98      42             2         5  \n",
            "4          120971          0.81      39             0         4  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2911067586.py:28: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X[col] = le.fit_transform(X[col])\n",
            "/tmp/ipython-input-2911067586.py:28: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X[col] = le.fit_transform(X[col])\n",
            "/tmp/ipython-input-2911067586.py:28: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X[col] = le.fit_transform(X[col])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "#  Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalize numeric features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 3️Build simple NN\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(32, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
        "    tf.keras.layers.Dense(16, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')  # Output: probability of being fake\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "history = model.fit(X_train_scaled, y_train,\n",
        "                    epochs=20,\n",
        "                    batch_size=32,\n",
        "                    validation_split=0.2)\n",
        "\n",
        "# Evaluate\n",
        "test_loss, test_acc = model.evaluate(X_test_scaled, y_test)\n",
        "print(\"\\nTest Accuracy:\", test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_siEwi8sXRIe",
        "outputId": "4f533e9f-3c29-4291-de24-d697a79cbbca"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.7207 - loss: 0.5434 - val_accuracy: 0.9962 - val_loss: 0.1163\n",
            "Epoch 2/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9980 - loss: 0.0693 - val_accuracy: 0.9987 - val_loss: 0.0217\n",
            "Epoch 3/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9996 - loss: 0.0153 - val_accuracy: 1.0000 - val_loss: 0.0086\n",
            "Epoch 4/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0072 - val_accuracy: 1.0000 - val_loss: 0.0045\n",
            "Epoch 5/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0036 - val_accuracy: 1.0000 - val_loss: 0.0028\n",
            "Epoch 6/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0025 - val_accuracy: 1.0000 - val_loss: 0.0019\n",
            "Epoch 7/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0015 - val_accuracy: 1.0000 - val_loss: 0.0014\n",
            "Epoch 8/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0012 - val_accuracy: 1.0000 - val_loss: 0.0010\n",
            "Epoch 9/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 9.8073e-04 - val_accuracy: 1.0000 - val_loss: 8.0638e-04\n",
            "Epoch 10/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 6.9260e-04 - val_accuracy: 1.0000 - val_loss: 6.4664e-04\n",
            "Epoch 11/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 5.3973e-04 - val_accuracy: 1.0000 - val_loss: 5.2664e-04\n",
            "Epoch 12/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 4.6618e-04 - val_accuracy: 1.0000 - val_loss: 4.3453e-04\n",
            "Epoch 13/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 4.1627e-04 - val_accuracy: 1.0000 - val_loss: 3.6552e-04\n",
            "Epoch 14/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 3.3487e-04 - val_accuracy: 1.0000 - val_loss: 3.1104e-04\n",
            "Epoch 15/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 2.5787e-04 - val_accuracy: 1.0000 - val_loss: 2.6731e-04\n",
            "Epoch 16/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 2.3740e-04 - val_accuracy: 1.0000 - val_loss: 2.3169e-04\n",
            "Epoch 17/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 2.0877e-04 - val_accuracy: 1.0000 - val_loss: 2.0203e-04\n",
            "Epoch 18/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 1.8440e-04 - val_accuracy: 1.0000 - val_loss: 1.7758e-04\n",
            "Epoch 19/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 1.5907e-04 - val_accuracy: 1.0000 - val_loss: 1.5695e-04\n",
            "Epoch 20/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 1.5225e-04 - val_accuracy: 1.0000 - val_loss: 1.3925e-04\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 1.1206e-04\n",
            "\n",
            "Test Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Example new record\n",
        "new_record = {\n",
        "    'reported_beneficiaries': 300,\n",
        "    'verified_beneficiaries': 290,\n",
        "    'beneficiary_gap': 10,\n",
        "    'funding_amount': 150000,\n",
        "    'impact_score': 0.95,\n",
        "    'ngo_id': 'NGO-001',\n",
        "    'project_type': 'Tree Plantation',\n",
        "    'location': 'Delhi'\n",
        "}\n",
        "\n",
        "# Convert to DataFrame\n",
        "new_df = pd.DataFrame([new_record])\n",
        "\n",
        "# Encode categorical features using the same LabelEncoders\n",
        "for col in ['ngo_id', 'project_type', 'location']:\n",
        "    new_df[col] = encoders[col].transform(new_df[col])\n",
        "\n",
        "# Scale numeric features using the same scaler\n",
        "new_scaled = scaler.transform(new_df)\n",
        "\n",
        "# Predict\n",
        "prediction = model.predict(new_scaled)\n",
        "prob = model.predict_proba(new_scaled)[0][0] if hasattr(model, \"predict_proba\") else prediction[0]\n",
        "\n",
        "print(\"Predicted class (0=real, 1=fake):\", int(prediction[0] > 0.5))\n",
        "print(\"Probability of being fake:\", float(prob))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQ_-W92VXRKf",
        "outputId": "1058ffcc-a474-4c77-ff7f-fac9b29325bf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 308ms/step\n",
            "Predicted class (0=real, 1=fake): 0\n",
            "Probability of being fake: 2.1979194571031258e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3612469322.py:29: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  print(\"Predicted class (0=real, 1=fake):\", int(prediction[0] > 0.5))\n",
            "/tmp/ipython-input-3612469322.py:30: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  print(\"Probability of being fake:\", float(prob))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example real NGO record\n",
        "real_record = {\n",
        "    'reported_beneficiaries': 250,\n",
        "    'verified_beneficiaries': 245,\n",
        "    'beneficiary_gap': 5,\n",
        "    'funding_amount': 120000,\n",
        "    'impact_score': 0.92,\n",
        "    'ngo_id': 'NGO-007',\n",
        "    'project_type': 'Education Drive',\n",
        "    'location': 'Bangalore'\n",
        "}\n",
        "\n",
        "# Convert to DataFrame\n",
        "new_df = pd.DataFrame([real_record])\n",
        "\n",
        "# Encode categorical features using the same LabelEncoders\n",
        "for col in ['ngo_id', 'project_type', 'location']:\n",
        "    new_df[col] = encoders[col].transform(new_df[col])\n",
        "\n",
        "# Scale numeric features\n",
        "new_scaled = scaler.transform(new_df)\n",
        "\n",
        "# Predict\n",
        "prediction = model.predict(new_scaled)\n",
        "prob = prediction[0][0]  # TensorFlow outputs an array\n",
        "\n",
        "print(\"Predicted class (0=real, 1=fake):\", int(prediction[0][0] > 0.5))\n",
        "print(\"Probability of being fake:\", float(prob))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ow_-MmYUXRMy",
        "outputId": "2310da50-6dfd-47e8-eafd-054af7b01ab8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
            "Predicted class (0=real, 1=fake): 0\n",
            "Probability of being fake: 1.0610242497932632e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fake_records = [\n",
        "    # huge gap, low impact score, high funding\n",
        "    {\n",
        "        'reported_beneficiaries': 900,\n",
        "        'verified_beneficiaries': 50,\n",
        "        'beneficiary_gap': 850,\n",
        "        'funding_amount': 350000,\n",
        "        'impact_score': 0.05,\n",
        "        'ngo_id': 'NGO-012',\n",
        "        'project_type': 'Food Distribution',\n",
        "        'location': 'Mumbai'\n",
        "    },\n",
        "    # reported high but verified zero, tiny impact score\n",
        "    {\n",
        "        'reported_beneficiaries': 700,\n",
        "        'verified_beneficiaries': 0,\n",
        "        'beneficiary_gap': 700,\n",
        "        'funding_amount': 300000,\n",
        "        'impact_score': 0.02,\n",
        "        'ngo_id': 'NGO-021',\n",
        "        'project_type': 'Health Camp',\n",
        "        'location': 'Delhi'\n",
        "    },\n",
        "    # moderate reported but verified very low, suspicious funding\n",
        "    {\n",
        "        'reported_beneficiaries': 450,\n",
        "        'verified_beneficiaries': 20,\n",
        "        'beneficiary_gap': 430,\n",
        "        'funding_amount': 280000,\n",
        "        'impact_score': 0.10,\n",
        "        'ngo_id': 'NGO-033',\n",
        "        'project_type': 'Education Drive',\n",
        "        'location': 'Kolkata'\n",
        "    },\n",
        "    # extreme gap + low score (looks fabricated)\n",
        "    {\n",
        "        'reported_beneficiaries': 1000,\n",
        "        'verified_beneficiaries': 10,\n",
        "        'beneficiary_gap': 990,\n",
        "        'funding_amount': 400000,\n",
        "        'impact_score': 0.01,\n",
        "        'ngo_id': 'NGO-005',\n",
        "        'project_type': 'Tree Plantation',\n",
        "        'location': 'Chennai'\n",
        "    },\n",
        "    # suspicious combination: medium reported but very low verified and low score\n",
        "    {\n",
        "        'reported_beneficiaries': 320,\n",
        "        'verified_beneficiaries': 5,\n",
        "        'beneficiary_gap': 315,\n",
        "        'funding_amount': 200000,\n",
        "        'impact_score': 0.08,\n",
        "        'ngo_id': 'NGO-018',\n",
        "        'project_type': 'Women Empowerment',\n",
        "        'location': 'Pune'\n",
        "    }\n",
        "]\n",
        "\n",
        "# Convert to DataFrame\n",
        "fake_df = pd.DataFrame(fake_records)\n",
        "\n",
        "# Encode categorical columns using the same LabelEncoders used during training\n",
        "for col in ['ngo_id', 'project_type', 'location']:\n",
        "    # If the encoder doesn't know a label, map to a fallback (most common label)\n",
        "    le = encoders[col]\n",
        "    def safe_transform(val):\n",
        "        if val in le.classes_:\n",
        "            return le.transform([val])[0]\n",
        "        else:\n",
        "            # fallback: use index 0 class (or you can choose another strategy)\n",
        "            return 0\n",
        "    fake_df[col] = fake_df[col].apply(safe_transform)\n",
        "\n",
        "# Scale numeric features using the same scaler\n",
        "fake_scaled = scaler.transform(fake_df)\n",
        "\n",
        "# Predict with the trained TensorFlow model\n",
        "preds = model.predict(fake_scaled)          # sigmoid outputs between 0 and 1\n",
        "pred_labels = (preds > 0.5).astype(int).ravel()\n",
        "\n",
        "# Show results\n",
        "fake_df['predicted_prob_fake'] = preds.ravel()\n",
        "fake_df['predicted_label'] = np.where(pred_labels==1, 'FAKE', 'REAL')\n",
        "\n",
        "print(\"\\nPredictions for fake-looking records:\\n\")\n",
        "print(fake_df[['reported_beneficiaries','verified_beneficiaries','beneficiary_gap',\n",
        "               'funding_amount','impact_score','ngo_id','project_type','location',\n",
        "               'predicted_prob_fake','predicted_label']])"
      ],
      "metadata": {
        "id": "kYKHiv_QXRPM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d0d80fb-887d-4119-9ab0-99c75f483ceb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 275ms/step\n",
            "\n",
            "Predictions for fake-looking records:\n",
            "\n",
            "   reported_beneficiaries  verified_beneficiaries  beneficiary_gap  \\\n",
            "0                     900                      50              850   \n",
            "1                     700                       0              700   \n",
            "2                     450                      20              430   \n",
            "3                    1000                      10              990   \n",
            "4                     320                       5              315   \n",
            "\n",
            "   funding_amount  impact_score  ngo_id  project_type  location  \\\n",
            "0          350000          0.05      21             2         5   \n",
            "1          300000          0.02      30             4         2   \n",
            "2          280000          0.10      42             1         4   \n",
            "3          400000          0.01      14             6         1   \n",
            "4          200000          0.08      27             7         7   \n",
            "\n",
            "   predicted_prob_fake predicted_label  \n",
            "0             1.000000            FAKE  \n",
            "1             1.000000            FAKE  \n",
            "2             0.999999            FAKE  \n",
            "3             1.000000            FAKE  \n",
            "4             0.999988            FAKE  \n"
          ]
        }
      ]
    }
  ]
}