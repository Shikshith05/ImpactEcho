{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPKPBxF4TJNrhlQJ5iYJI6U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shikshith05/ImpactEcho/blob/main/impactechomodel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kyYDSjuWp5z"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "def generate_impact_data(num_records=10000, fake_rate=0.1):\n",
        "    \"\"\"\n",
        "    Generate synthetic NGO impact verification data for ImpactEcho AI.\n",
        "\n",
        "    Args:\n",
        "        num_records (int): Number of total records to generate.\n",
        "        fake_rate (float): Fraction of fake or manipulated records.\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: Generated dataset with authenticity labels.\n",
        "    \"\"\"\n",
        "\n",
        "    num_fake = int(num_records * fake_rate)\n",
        "    num_real = num_records - num_fake\n",
        "\n",
        "    print(f\"Generating {num_records:,} NGO impact records ({num_real:,} real, {num_fake:,} fake)\")\n",
        "\n",
        "    ngo_ids = [f\"NGO-{i:03d}\" for i in range(1, 51)]\n",
        "    project_types = [\"Tree Plantation\", \"Education Drive\", \"Health Camp\", \"Clean Water\", \"Women Empowerment\", \"Food Distribution\"]\n",
        "    locations = [\"Delhi\", \"Bangalore\", \"Hyderabad\", \"Mumbai\", \"Kolkata\", \"Chennai\", \"Pune\"]\n",
        "\n",
        "    real_data, fake_data = [], []\n",
        "    base_time = datetime(2025, 9, 1, 10, 0, 0)\n",
        "\n",
        "    # ‚úÖ Generate real records\n",
        "    for _ in range(num_real):\n",
        "        ngo_id = random.choice(ngo_ids)\n",
        "        project = random.choice(project_types)\n",
        "        location = random.choice(locations)\n",
        "        timestamp = base_time + timedelta(days=random.randint(0, 60))\n",
        "\n",
        "        beneficiaries_reported = random.randint(80, 500)\n",
        "        verified_beneficiaries = beneficiaries_reported - random.randint(0, 10)\n",
        "        impact_score = round(random.uniform(0.8, 1.0), 2)\n",
        "        funding_amount = random.randint(50000, 300000)\n",
        "\n",
        "        media_link = f\"https://impactecho.org/media/{ngo_id}_{project.replace(' ', '_')}.jpg\"\n",
        "\n",
        "        real_data.append([\n",
        "            timestamp.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "            ngo_id,\n",
        "            project,\n",
        "            location,\n",
        "            media_link,\n",
        "            beneficiaries_reported,\n",
        "            verified_beneficiaries,\n",
        "            funding_amount,\n",
        "            impact_score,\n",
        "            0  # is_fake\n",
        "        ])\n",
        "\n",
        "    # ‚ö†Ô∏è Generate fake records\n",
        "    for _ in range(num_fake):\n",
        "        ngo_id = random.choice(ngo_ids + [f\"FAKE-{i:03d}\" for i in range(51, 61)])\n",
        "        project = random.choice(project_types + [\"Ghost Project\", \"Phantom Relief\"])\n",
        "        location = random.choice(locations + [\"Unknown\", \"Remote Village\", \"N/A\"])\n",
        "        timestamp = base_time + timedelta(days=random.randint(0, 60))\n",
        "\n",
        "        # Fake or inconsistent metrics\n",
        "        beneficiaries_reported = random.randint(100, 1000)\n",
        "        verified_beneficiaries = max(0, beneficiaries_reported - random.randint(100, 900))\n",
        "        impact_score = round(random.uniform(0.0, 0.4), 2)\n",
        "        funding_amount = random.randint(100000, 400000)\n",
        "\n",
        "        # Broken or reused media\n",
        "        media_link = random.choice([\n",
        "            f\"https://impactecho.org/media/fake_{random.randint(1,999)}.jpg\",\n",
        "            \"https://imgur.com/fakeproof123\",\n",
        "            \"https://drive.google.com/brokenlink\"\n",
        "        ])\n",
        "\n",
        "        fake_data.append([\n",
        "            timestamp.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "            ngo_id,\n",
        "            project,\n",
        "            location,\n",
        "            media_link,\n",
        "            beneficiaries_reported,\n",
        "            verified_beneficiaries,\n",
        "            funding_amount,\n",
        "            impact_score,\n",
        "            1  # is_fake\n",
        "        ])\n",
        "\n",
        "    # Combine + shuffle\n",
        "    df = pd.DataFrame(real_data + fake_data, columns=[\n",
        "        \"timestamp\", \"ngo_id\", \"project_type\", \"location\", \"media_link\",\n",
        "        \"reported_beneficiaries\", \"verified_beneficiaries\", \"funding_amount\",\n",
        "        \"impact_score\", \"is_fake\"\n",
        "    ])\n",
        "\n",
        "    df = df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "    print(f\"\\n‚úÖ Data generation complete!\")\n",
        "    print(f\"Total records: {len(df):,}\")\n",
        "    print(f\"Fake records: {df['is_fake'].sum():,}\")\n",
        "    print(f\"Fake rate: {df['is_fake'].mean() * 100:.2f}%\")\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def save_to_json(df, filename=\"impact_data\"):\n",
        "    df.to_json(f\"{filename}.json\", orient=\"records\", indent=2)\n",
        "    df.to_json(f\"{filename}.jsonl\", orient=\"records\", lines=True)\n",
        "    print(f\"üíæ Saved to {filename}.json and {filename}.jsonl\")\n",
        "\n",
        "\n",
        "def display_sample(df, n=5):\n",
        "    print(f\"\\nüìä Showing {n} random samples:\\n\")\n",
        "    print(df.sample(n).to_string(index=False))\n",
        "    print(\"\\nüìà Summary:\\n\", df.describe())\n",
        "\n",
        "\n",
        "if __name__ == \"_main_\":\n",
        "    impact_df = generate_impact_data(num_records=5000, fake_rate=0.1)\n",
        "    display_sample(impact_df)\n",
        "    save_to_json(impact_df, \"impact_data_full\")\n",
        "\n",
        "    # Split for training/testing AI model\n",
        "    train_df = impact_df.sample(frac=0.8, random_state=42)\n",
        "    test_df = impact_df.drop(train_df.index)\n",
        "\n",
        "    save_to_json(train_df, \"impact_data_train\")\n",
        "    save_to_json(test_df, \"impact_data_test\")\n",
        "\n",
        "    print(\"\\nüìÇ Training records:\", len(train_df))\n",
        "    print(\"üìÇ Testing records:\", len(test_df))\n",
        "    print(\"\\n‚úÖ ImpactEcho synthetic dataset created successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "impact_df = generate_impact_data(num_records=5000, fake_rate=0.1)\n",
        "display_sample(impact_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRNVF_gSXRGU",
        "outputId": "9a47ee69-299b-427a-b6f5-0955d4bc4914"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating 5,000 NGO impact records (4,500 real, 500 fake)\n",
            "\n",
            "‚úÖ Data generation complete!\n",
            "Total records: 5,000\n",
            "Fake records: 500\n",
            "Fake rate: 10.00%\n",
            "\n",
            "üìä Showing 5 random samples:\n",
            "\n",
            "          timestamp  ngo_id      project_type  location                                                 media_link  reported_beneficiaries  verified_beneficiaries  funding_amount  impact_score  is_fake\n",
            "2025-09-13 10:00:00 NGO-038 Food Distribution Hyderabad https://impactecho.org/media/NGO-038_Food_Distribution.jpg                     406                     406          181149          0.92        0\n",
            "2025-09-05 10:00:00 NGO-012   Tree Plantation   Kolkata   https://impactecho.org/media/NGO-012_Tree_Plantation.jpg                     215                     215          140693          0.99        0\n",
            "2025-10-02 10:00:00 NGO-031   Education Drive Hyderabad   https://impactecho.org/media/NGO-031_Education_Drive.jpg                     125                     115          251635          0.90        0\n",
            "2025-10-10 10:00:00 NGO-016 Women Empowerment   Chennai https://impactecho.org/media/NGO-016_Women_Empowerment.jpg                     234                     228          238269          0.81        0\n",
            "2025-10-10 10:00:00 NGO-015 Food Distribution Bangalore https://impactecho.org/media/NGO-015_Food_Distribution.jpg                     295                     286           85056          0.81        0\n",
            "\n",
            "üìà Summary:\n",
            "        reported_beneficiaries  verified_beneficiaries  funding_amount  \\\n",
            "count             5000.000000             5000.000000     5000.000000   \n",
            "mean               316.578600              274.426200   183479.011000   \n",
            "std                160.708836              137.910676    76488.708106   \n",
            "min                 80.000000                0.000000    50094.000000   \n",
            "25%                191.000000              164.000000   118709.250000   \n",
            "50%                303.500000              276.000000   184028.000000   \n",
            "75%                414.250000              388.000000   245109.250000   \n",
            "max                999.000000              859.000000   399838.000000   \n",
            "\n",
            "       impact_score     is_fake  \n",
            "count   5000.000000  5000.00000  \n",
            "mean       0.830452     0.10000  \n",
            "std        0.220736     0.30003  \n",
            "min        0.000000     0.00000  \n",
            "25%        0.830000     0.00000  \n",
            "50%        0.890000     0.00000  \n",
            "75%        0.940000     0.00000  \n",
            "max        1.000000     1.00000  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Copy dataframe to avoid modifying original\n",
        "df = impact_df.copy()\n",
        "\n",
        "# 1Ô∏è‚É£ Derived feature\n",
        "df['beneficiary_gap'] = df['reported_beneficiaries'] - df['verified_beneficiaries']\n",
        "\n",
        "# 2Ô∏è‚É£ Features & labels\n",
        "features = [\n",
        "    'reported_beneficiaries',\n",
        "    'verified_beneficiaries',\n",
        "    'beneficiary_gap',\n",
        "    'funding_amount',\n",
        "    'impact_score',\n",
        "    'ngo_id',\n",
        "    'project_type',\n",
        "    'location'\n",
        "]\n",
        "X = df[features]\n",
        "y = df['is_fake']\n",
        "\n",
        "# 3Ô∏è‚É£ Encode categorical columns\n",
        "categorical_cols = ['ngo_id', 'project_type', 'location']\n",
        "encoders = {}\n",
        "for col in categorical_cols:\n",
        "    le = LabelEncoder()\n",
        "    X[col] = le.fit_transform(X[col])\n",
        "    encoders[col] = le\n",
        "\n",
        "# Check preprocessed data\n",
        "print(\"Preprocessed feature sample:\")\n",
        "print(X.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJbDre1wYdam",
        "outputId": "076b4bbf-b395-4797-9ba0-3c4127f4d537"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessed feature sample:\n",
            "   reported_beneficiaries  verified_beneficiaries  beneficiary_gap  \\\n",
            "0                     181                     179                2   \n",
            "1                     374                     373                1   \n",
            "2                     298                     293                5   \n",
            "3                     195                     186                9   \n",
            "4                     707                     382              325   \n",
            "\n",
            "   funding_amount  impact_score  ngo_id  project_type  location  \n",
            "0          110312          0.97      59             2         4  \n",
            "1          266369          0.99      14             2         4  \n",
            "2          179945          0.88      38             2         2  \n",
            "3           83313          0.85      47             7         0  \n",
            "4          110746          0.14      57             1         1  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2911067586.py:28: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X[col] = le.fit_transform(X[col])\n",
            "/tmp/ipython-input-2911067586.py:28: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X[col] = le.fit_transform(X[col])\n",
            "/tmp/ipython-input-2911067586.py:28: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X[col] = le.fit_transform(X[col])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 1Ô∏è‚É£ Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 2Ô∏è‚É£ Normalize numeric features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 3Ô∏è‚É£ Build simple NN\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(32, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
        "    tf.keras.layers.Dense(16, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')  # Output: probability of being fake\n",
        "])\n",
        "\n",
        "# 4Ô∏è‚É£ Compile model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# 5Ô∏è‚É£ Train model\n",
        "history = model.fit(X_train_scaled, y_train,\n",
        "                    epochs=20,\n",
        "                    batch_size=32,\n",
        "                    validation_split=0.2)\n",
        "\n",
        "# 6Ô∏è‚É£ Evaluate\n",
        "test_loss, test_acc = model.evaluate(X_test_scaled, y_test)\n",
        "print(\"\\nTest Accuracy:\", test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_siEwi8sXRIe",
        "outputId": "962950b3-915d-416d-c12d-f5e4027cd538"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m100/100\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9491 - loss: 0.3299 - val_accuracy: 0.9987 - val_loss: 0.0482\n",
            "Epoch 2/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9978 - loss: 0.0415 - val_accuracy: 1.0000 - val_loss: 0.0114\n",
            "Epoch 3/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9999 - loss: 0.0114 - val_accuracy: 1.0000 - val_loss: 0.0049\n",
            "Epoch 4/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0053 - val_accuracy: 1.0000 - val_loss: 0.0026\n",
            "Epoch 5/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0031 - val_accuracy: 1.0000 - val_loss: 0.0016\n",
            "Epoch 6/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0021 - val_accuracy: 1.0000 - val_loss: 0.0011\n",
            "Epoch 7/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0014 - val_accuracy: 1.0000 - val_loss: 7.7743e-04\n",
            "Epoch 8/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 1.0000 - val_loss: 5.8136e-04\n",
            "Epoch 9/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 8.4434e-04 - val_accuracy: 1.0000 - val_loss: 4.4829e-04\n",
            "Epoch 10/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 6.7707e-04 - val_accuracy: 1.0000 - val_loss: 3.5538e-04\n",
            "Epoch 11/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 5.0379e-04 - val_accuracy: 1.0000 - val_loss: 2.8856e-04\n",
            "Epoch 12/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 4.4276e-04 - val_accuracy: 1.0000 - val_loss: 2.3756e-04\n",
            "Epoch 13/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 3.7219e-04 - val_accuracy: 1.0000 - val_loss: 1.9851e-04\n",
            "Epoch 14/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 2.8272e-04 - val_accuracy: 1.0000 - val_loss: 1.6804e-04\n",
            "Epoch 15/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 2.4497e-04 - val_accuracy: 1.0000 - val_loss: 1.4401e-04\n",
            "Epoch 16/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 2.1462e-04 - val_accuracy: 1.0000 - val_loss: 1.2401e-04\n",
            "Epoch 17/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 1.8244e-04 - val_accuracy: 1.0000 - val_loss: 1.0769e-04\n",
            "Epoch 18/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 1.6808e-04 - val_accuracy: 1.0000 - val_loss: 9.4181e-05\n",
            "Epoch 19/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 1.5143e-04 - val_accuracy: 1.0000 - val_loss: 8.2969e-05\n",
            "Epoch 20/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 1.2743e-04 - val_accuracy: 1.0000 - val_loss: 7.3234e-05\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 1.6829e-04  \n",
            "\n",
            "Test Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Example new record\n",
        "new_record = {\n",
        "    'reported_beneficiaries': 300,\n",
        "    'verified_beneficiaries': 290,\n",
        "    'beneficiary_gap': 10,\n",
        "    'funding_amount': 150000,\n",
        "    'impact_score': 0.95,\n",
        "    'ngo_id': 'NGO-001',\n",
        "    'project_type': 'Tree Plantation',\n",
        "    'location': 'Delhi'\n",
        "}\n",
        "\n",
        "# Convert to DataFrame\n",
        "new_df = pd.DataFrame([new_record])\n",
        "\n",
        "# Encode categorical features using the same LabelEncoders\n",
        "for col in ['ngo_id', 'project_type', 'location']:\n",
        "    new_df[col] = encoders[col].transform(new_df[col])\n",
        "\n",
        "# Scale numeric features using the same scaler\n",
        "new_scaled = scaler.transform(new_df)\n",
        "\n",
        "# Predict\n",
        "prediction = model.predict(new_scaled)\n",
        "prob = model.predict_proba(new_scaled)[0][0] if hasattr(model, \"predict_proba\") else prediction[0]\n",
        "\n",
        "print(\"Predicted class (0=real, 1=fake):\", int(prediction[0] > 0.5))\n",
        "print(\"Probability of being fake:\", float(prob))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQ_-W92VXRKf",
        "outputId": "67ef3b33-ea7f-47c0-9b38-1724298e9e77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
            "Predicted class (0=real, 1=fake): 0\n",
            "Probability of being fake: 7.870806257415097e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3612469322.py:29: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  print(\"Predicted class (0=real, 1=fake):\", int(prediction[0] > 0.5))\n",
            "/tmp/ipython-input-3612469322.py:30: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  print(\"Probability of being fake:\", float(prob))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example real NGO record\n",
        "real_record = {\n",
        "    'reported_beneficiaries': 250,\n",
        "    'verified_beneficiaries': 245,\n",
        "    'beneficiary_gap': 5,\n",
        "    'funding_amount': 120000,\n",
        "    'impact_score': 0.92,\n",
        "    'ngo_id': 'NGO-007',\n",
        "    'project_type': 'Education Drive',\n",
        "    'location': 'Bangalore'\n",
        "}\n",
        "\n",
        "# Convert to DataFrame\n",
        "new_df = pd.DataFrame([real_record])\n",
        "\n",
        "# Encode categorical features using the same LabelEncoders\n",
        "for col in ['ngo_id', 'project_type', 'location']:\n",
        "    new_df[col] = encoders[col].transform(new_df[col])\n",
        "\n",
        "# Scale numeric features\n",
        "new_scaled = scaler.transform(new_df)\n",
        "\n",
        "# Predict\n",
        "prediction = model.predict(new_scaled)\n",
        "prob = prediction[0][0]  # TensorFlow outputs an array\n",
        "\n",
        "print(\"Predicted class (0=real, 1=fake):\", int(prediction[0][0] > 0.5))\n",
        "print(\"Probability of being fake:\", float(prob))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ow_-MmYUXRMy",
        "outputId": "18edebea-74ff-4201-93be-3c253ffe7104"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "Predicted class (0=real, 1=fake): 0\n",
            "Probability of being fake: 4.0727074519963935e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fake_records = [\n",
        "    # huge gap, low impact score, high funding\n",
        "    {\n",
        "        'reported_beneficiaries': 900,\n",
        "        'verified_beneficiaries': 50,\n",
        "        'beneficiary_gap': 850,\n",
        "        'funding_amount': 350000,\n",
        "        'impact_score': 0.05,\n",
        "        'ngo_id': 'NGO-012',\n",
        "        'project_type': 'Food Distribution',\n",
        "        'location': 'Mumbai'\n",
        "    },\n",
        "    # reported high but verified zero, tiny impact score\n",
        "    {\n",
        "        'reported_beneficiaries': 700,\n",
        "        'verified_beneficiaries': 0,\n",
        "        'beneficiary_gap': 700,\n",
        "        'funding_amount': 300000,\n",
        "        'impact_score': 0.02,\n",
        "        'ngo_id': 'NGO-021',\n",
        "        'project_type': 'Health Camp',\n",
        "        'location': 'Delhi'\n",
        "    },\n",
        "    # moderate reported but verified very low, suspicious funding\n",
        "    {\n",
        "        'reported_beneficiaries': 450,\n",
        "        'verified_beneficiaries': 20,\n",
        "        'beneficiary_gap': 430,\n",
        "        'funding_amount': 280000,\n",
        "        'impact_score': 0.10,\n",
        "        'ngo_id': 'NGO-033',\n",
        "        'project_type': 'Education Drive',\n",
        "        'location': 'Kolkata'\n",
        "    },\n",
        "    # extreme gap + low score (looks fabricated)\n",
        "    {\n",
        "        'reported_beneficiaries': 1000,\n",
        "        'verified_beneficiaries': 10,\n",
        "        'beneficiary_gap': 990,\n",
        "        'funding_amount': 400000,\n",
        "        'impact_score': 0.01,\n",
        "        'ngo_id': 'NGO-005',\n",
        "        'project_type': 'Tree Plantation',\n",
        "        'location': 'Chennai'\n",
        "    },\n",
        "    # suspicious combination: medium reported but very low verified and low score\n",
        "    {\n",
        "        'reported_beneficiaries': 320,\n",
        "        'verified_beneficiaries': 5,\n",
        "        'beneficiary_gap': 315,\n",
        "        'funding_amount': 200000,\n",
        "        'impact_score': 0.08,\n",
        "        'ngo_id': 'NGO-018',\n",
        "        'project_type': 'Women Empowerment',\n",
        "        'location': 'Pune'\n",
        "    }\n",
        "]\n",
        "\n",
        "# Convert to DataFrame\n",
        "fake_df = pd.DataFrame(fake_records)\n",
        "\n",
        "# Encode categorical columns using the same LabelEncoders used during training\n",
        "for col in ['ngo_id', 'project_type', 'location']:\n",
        "    # If the encoder doesn't know a label, map to a fallback (most common label)\n",
        "    le = encoders[col]\n",
        "    def safe_transform(val):\n",
        "        if val in le.classes_:\n",
        "            return le.transform([val])[0]\n",
        "        else:\n",
        "            # fallback: use index 0 class (or you can choose another strategy)\n",
        "            return 0\n",
        "    fake_df[col] = fake_df[col].apply(safe_transform)\n",
        "\n",
        "# Scale numeric features using the same scaler\n",
        "fake_scaled = scaler.transform(fake_df)\n",
        "\n",
        "# Predict with the trained TensorFlow model\n",
        "preds = model.predict(fake_scaled)          # sigmoid outputs between 0 and 1\n",
        "pred_labels = (preds > 0.5).astype(int).ravel()\n",
        "\n",
        "# Show results\n",
        "fake_df['predicted_prob_fake'] = preds.ravel()\n",
        "fake_df['predicted_label'] = np.where(pred_labels==1, 'FAKE', 'REAL')\n",
        "\n",
        "print(\"\\nPredictions for fake-looking records:\\n\")\n",
        "print(fake_df[['reported_beneficiaries','verified_beneficiaries','beneficiary_gap',\n",
        "               'funding_amount','impact_score','ngo_id','project_type','location',\n",
        "               'predicted_prob_fake','predicted_label']])"
      ],
      "metadata": {
        "id": "kYKHiv_QXRPM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fa382d8-16b8-4e11-866f-9cd486cb027a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
            "\n",
            "Predictions for fake-looking records:\n",
            "\n",
            "   reported_beneficiaries  verified_beneficiaries  beneficiary_gap  \\\n",
            "0                     900                      50              850   \n",
            "1                     700                       0              700   \n",
            "2                     450                      20              430   \n",
            "3                    1000                      10              990   \n",
            "4                     320                       5              315   \n",
            "\n",
            "   funding_amount  impact_score  ngo_id  project_type  location  \\\n",
            "0          350000          0.05      21             2         5   \n",
            "1          300000          0.02      30             4         2   \n",
            "2          280000          0.10      42             1         4   \n",
            "3          400000          0.01      14             6         1   \n",
            "4          200000          0.08      27             7         7   \n",
            "\n",
            "   predicted_prob_fake predicted_label  \n",
            "0             1.000000            FAKE  \n",
            "1             1.000000            FAKE  \n",
            "2             0.999999            FAKE  \n",
            "3             1.000000            FAKE  \n",
            "4             0.999977            FAKE  \n"
          ]
        }
      ]
    }
  ]
}